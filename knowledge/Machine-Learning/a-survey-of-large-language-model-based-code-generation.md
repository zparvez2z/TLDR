---
source_url: https://arxiv.org/html/2411.00041v1
author: Zejun Zhang, Yuchi Tian, Zhenyu Yang, Zhaowei Zhang, Ruilong Li, Yisheng Xiao, Ge Li, Zhi Jin
date: 31-10-2024
---

# A Survey of Large Language Model-based Code Generation

This paper presents a comprehensive survey on the field of code generation using Large Language Models (LLMs). It traces the historical progression from traditional techniques to modern LLM-based approaches, highlighting the significant impact of models like Codex and AlphaCode. The survey systematically organizes the current research landscape into three core components: data, models, and evaluation. It delves into the specifics of data sourcing and preprocessing, various model architectures and training strategies, and the metrics used for performance assessment, concluding by identifying key challenges and future research directions.

*   **Evolution of Code Generation:** The field has shifted from traditional, rule-based methods to the more powerful and flexible LLM-driven paradigm.
*   **Three Pillars of Research:** The survey structures the domain into three key areas: data (sourcing, cleaning, benchmarks), models (architectures, training, fine-tuning), and evaluation (metrics like pass@k, benchmarks).
*   **Data is Crucial:** The quality and diversity of training data, often sourced from platforms like GitHub, are fundamental to building effective code generation models.
*   **Model Architectures:** The paper reviews various model architectures, including encoder-decoder and decoder-only transformers, and discusses different pre-training and fine-tuning strategies.
*   **Challenges and Future Directions:** Key challenges include improving the models' reasoning abilities, handling long-context codebases, and ensuring the security and reliability of the generated code.