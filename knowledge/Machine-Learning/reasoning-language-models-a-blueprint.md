---
source_url: https://arxiv.org/pdf/2501.11223
author: Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, Tomasz Sternal, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Łukasz Flis, Hannes Eberhard, Zixuan Chen, Hubert Niewiadomski, Torsten Hoefler
date: 11-06-2025
---

# Reasoning Language Models: A Blueprint

Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), represent a significant advancement in AI by integrating advanced reasoning mechanisms with large language models (LLMs). This paper presents a comprehensive blueprint for constructing RLMs, aiming to democratize these powerful technologies. The authors propose a modular framework that breaks down RLM components, including reasoning structures (like chains, trees, and graphs), reasoning strategies (such as Monte Carlo Tree Search), and reinforcement learning concepts. This work aims to demystify RLM construction and foster innovation by lowering the barriers to their design and implementation.

- **Blueprint for RLMs:** The paper introduces a modular and comprehensive blueprint to build, analyze, and experiment with RLMs.
- **Demystification and Democratization:** The work aims to make advanced AI reasoning capabilities more accessible, mitigating the gap between "rich AI" and "poor AI".
- **Core Components:** RLMs extend LLMs by incorporating components like search heuristics and reinforcement learning to enhance problem-solving.
- **Modular Framework (x1):** The authors introduce x1, a modular implementation, to facilitate rapid prototyping and experimentation with RLM designs.
- **Unified Approach:** The proposed blueprint is shown to be a unifying framework capable of expressing various existing RLM schemes as special cases.