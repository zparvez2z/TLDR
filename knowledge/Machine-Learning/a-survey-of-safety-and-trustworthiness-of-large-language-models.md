---
source_url: https://arxiv.org/pdf/2411.00041
author: Zhiyuan Wang, Yutao Wang, Yequan Wang, Yifei Wang, Kexin Huang, Yating Zhang, Changlong Sun, Fei Mi, Weidong Xiao, Shuaiqiang Wang, Sang-Wook Kim, Dawei Yin
date: 31-10-2024
---

# A Survey of Safety and Trustworthiness of Large Language Models

This survey provides a comprehensive overview of the safety and trustworthiness challenges posed by the rapid advancement of Large Language Models (LLMs). The authors introduce a three-layer taxonomy to systematically categorize these issues, encompassing foundational model safety, application system security, and social security. The paper reviews existing research on identifying and mitigating risks such as misinformation, toxicity, privacy leakage, and adversarial attacks. It concludes by highlighting open challenges and suggesting future research directions to foster the development of safer and more reliable LLM technologies.

- **Three-Layer Taxonomy:** Proposes a structured framework for understanding LLM safety, divided into foundational model safety, application system security, and social security.
- **Risk Identification:** Catalogs a wide range of potential harms, including the generation of harmful content, privacy violations, and susceptibility to manipulation.
- **Mitigation Techniques:** Discusses current methods for enhancing LLM safety, such as value alignment, red teaming, and robust evaluation protocols.
- **Open Challenges:** Identifies key unresolved issues, including the difficulty of comprehensive risk assessment and the scalability of safety measures.
- **Future Directions:** Outlines promising areas for future research, emphasizing the need for more robust, interpretable, and adaptable safety solutions.